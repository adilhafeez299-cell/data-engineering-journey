# DATA ENGINEERING CAREER TRANSITION
## Master Plan v1.0 - Single Authoritative Source

**Status:** Week 2 In Progress (November 29â€“December 5, 2025)
**Timeline:** November 2025 â†’ July 2027 (18-24 months)
**Current Role:** Junior Systems Engineer at Atlas Technica (Â£35k)
**Immediate Target:** Junior Software Engineer / Platform Engineer (Â£38k-Â£50k) by January 2026
**Long-term Target:** Data Engineer (Â£60k-Â£75k) by mid-2027
**Location:** London, UK

---

> Authoritative Alignment (updated)
> - Weeks run Saturday â†’ Friday; Week 1 = Nov 22â€“28, 2025
> - 4 Phases total:
>   1) Phase 1: Python Foundations (Weeks 1â€“8, Nov 22â€“Jan 12)
>   2) Phase 2: SQL Mastery (Weeks 9â€“16, Jan 18â€“Mar 9)
>   3) Phase 3: AWS & Cloud (Weeks 17â€“24, Mar 15â€“May 10)
>   4) Phase 4: Databricks + Capstone (Weeks 25â€“36, May 17â€“Aug 3)
> - AWS DE Associate: May 2026 (Week 24, target May 10, 2026)
> - Capstone: Commodity Price Monitoring System (Weeks 25â€“36)

---

## EXECUTIVE SUMMARY

### The Mission
Transform from Systems Engineer to Data Engineer through strategic two-phase approach:
- **Phase 1:** Secure Junior Software Engineer/Platform Engineer role (Jan 2026)
- **Phase 2:** Build engineering foundation on the job (18-24 months)
- **Phase 3:** Transition to Data Engineer specialization (mid-2027)
- Foundation skills learned properly with mentorship, not rushed weekend self-study

### Why This Works (Revised Strategy)
1. **Engineering Role First**: Target Junior SWE/Platform Engineer (Â£38-50k) by Jan 2026
   - Learn to code 40 hours/week (paid to learn)
   - Professional mentorship and code reviews
   - Proper engineering culture and best practices

2. **Build Strong Foundation**: 18-24 months in engineering role (Jan 2026 - Jul 2027)
   - Master Python/backend development through actual work
   - Learn cloud infrastructure, CI/CD, testing, Git workflows
   - Weekend study supplements with data-specific skills (SQL, Spark, data pipelines)

3. **Specialize in Data**: Mid-2027 transition to Data Engineer
   - Apply with REAL engineering experience (not just self-study)
   - Certifications and portfolio projects complement job experience
   - Target Â£60k-Â£75k (realistic for engineer with 18-24 months experience)

4. **Why Not Cloud Support**:
   - Support roles = lateral move with limited growth
   - Engineering roles = career trajectory and proper development skills
   - Better foundation for long-term data engineering career

5. **Sustainable Timeline**:
   - Not rushing to "Data Engineer" title in 9 months
   - Building genuine competence over 18-24 months
   - Higher probability of success, lower burnout risk

### What You'll Have by Mid-2027

âœ… **Job Experience (Primary Asset):**
- 18-24 months as Junior Software Engineer / Platform Engineer
- Real production code experience with professional mentorship
- Code reviews, testing, CI/CD, cloud infrastructure
- Proven ability to work in engineering teams

âœ… **Technical Foundation (From Job + Weekend Study):**
- **From Job:** Python/backend development, Git workflows, testing, cloud platforms, CI/CD
- **From Weekends:** SQL mastery, data pipelines, PySpark, data modeling
- Combined: Strong engineering foundation + data-specific skills

âœ… **Certifications (Targeted, Not Rushed):**
- AWS Certified Developer Associate OR AWS Solutions Architect (earned during job, 2026)
- Optional: Databricks Data Engineer Associate (if time permits, 2027)
- Focus: Depth over quantity, complement job experience

âœ… **Portfolio Projects (Quality Over Quantity):**
- 3-4 well-built projects showing engineering + data skills
- Project 1: Python Automation Tool (complete by Jan 2026)
- Project 2: Data Pipeline or ETL project (2026, using job skills)
- Project 3: End-to-end data project with cloud deployment (2027)
- Each project: Production-quality, well-documented, demonstrates real skills

---

## DUAL-TRACK STRATEGY (REVISED)

### TRACK 1: Job Search (Immediate Priority - UPDATED)
**Goal:** Secure Junior Software Engineer or Platform Engineer role by January 2026

**Target Roles (PRIORITY ORDER):**
1. **Junior Software Engineer (Backend)** â­â­â­ (Best for learning to code)
   - Python/Java/Go backend development
   - RESTful APIs, databases, testing
   - Engineering culture with mentorship
   - Salary: Â£38k-Â£48k
   - **Why:** Learn to code properly, 40hrs/week with mentorship

2. **Platform Engineer / DevOps Engineer** â­â­ (Best for infrastructure)
   - Cloud infrastructure (AWS/Azure/GCP)
   - CI/CD, containerization (Docker/Kubernetes), IaC (Terraform)
   - Automation and scripting
   - Salary: Â£40k-Â£50k
   - **Why:** Infrastructure foundation, cloud skills, automation experience

3. **Cloud Support Engineer** â­ (Backup only if no engineering offers)
   - Only consider if no Junior SWE/Platform Engineer offers by February 2026
   - Uses Azure/M365/Intune expertise
   - Less reactive than MSP
   - **Why:** Lateral move with limited growth - avoid if possible

**Application Strategy:**
- **Nov-Dec 2025:**
  - Update CV for engineering roles (highlight Python learning, projects, cloud experience)
  - LinkedIn optimization for Junior SWE/Platform Engineer keywords
  - Target 20-30 companies with strong engineering culture
  - Apply to 5-10 quality roles (no mass applications)

- **Target:** Engineering offer by January 2026
- **Plan:** Stay 18-24 months, learn properly, then transition to Data Engineer
- **Flexibility:** Willing to invest time in strong engineering foundation over quick title bump

**Why Engineering Roles Over Support:**
- Engineering = career trajectory, mentorship, real development experience
- Support = lateral move, limited growth, mostly troubleshooting
- 18-24 months learning foundation > 9-month weekend sprint
- By 2027, you'll be STRONG engineer ready to specialize in data

### TRACK 2: Learning (Continuous - REVISED FOCUS)
**Goal:** Build engineering fundamentals to support Junior SWE/Platform Engineer role, then supplement with data-specific skills

**NEW APPROACH:**
- **Before Jan 2026:** Focus on engineering fundamentals for job applications
- **Jan 2026-Jul 2027:** Learn on the job (40hrs/week) + weekend data-specific skills (5-8hrs/week)
- **Mid-2027:** Apply to Data Engineer roles with real experience + targeted portfolio

**Phase 1: Engineering Fundamentals (Nov 2025 - Jan 2026)**
**Goal:** Prepare for Junior SWE/Platform Engineer interviews

- **Python Fundamentals:**
  - Resource: Bogdan Python Course (O'Reilly) - Chapters 1-54+
  - Current: 25% complete (Chapter 26)
  - Target by Jan 2026: 60-70% (focus on core concepts, not exhaustive completion)
  - Project: Python automation tool (complete by Jan 2026)
  - Hours: 10-13/week weekends

- **Supplementary (As Time Permits):**
  - Git/GitHub: Practice commits, branches, pull requests
  - Basic algorithms/data structures: For technical interviews
  - REST APIs: Basic understanding for backend interviews
  - Focus: Job-ready skills, not certification cramming

**Phase 2: On-the-Job Learning + Data Skills Supplement (Jan 2026 - Jul 2027)**
**Goal:** Master engineering through work, supplement weekends with data-specific skills

- **Jan-Mar 2026 (Months 1-3):** Onboarding + stabilization
  - Focus 100% on new engineering job
  - Minimal weekend study (0-3 hours if any)
  - Learn codebase, team workflows, engineering practices
  - No pressure to study - job learning is the priority

- **Apr-Sep 2026 (Months 4-9):** Stable + SQL foundation
  - Weekend study: 5-8 hours/week
  - Resource: Mode Analytics SQL Tutorial (complete)
  - LeetCode SQL problems: 30-50 problems
  - Project 2: Financial dataset analysis (portfolio piece)
  - Parallel: Apply job engineering skills to data context

- **Oct 2026-Mar 2027 (Months 10-15):** Data tools + certification
  - Weekend study: 5-8 hours/week
  - Resource: AWS Certified Developer or Solutions Architect prep
  - Target: One AWS certification (complements job experience)
  - Project 3: Cloud-based data pipeline (AWS/Azure depending on job)
  - Focus: Bridge engineering skills to data engineering

- **Apr-Jul 2027 (Months 16-19):** Data Engineer transition prep
  - Weekend study: 8-10 hours/week
  - PySpark/Databricks fundamentals
  - Optional: Databricks Associate cert (if time/energy permits)
  - Project 4: End-to-end data engineering capstone
  - Mid-2027: Launch Data Engineer job search

---

## WEEKLY SCHEDULE (REVISED)

### Phase 1: Before Engineering Job (Nov 2025 - Jan 2026)

**Standard Work Week (Mon-Fri):**
- **Monday:** Gym only, no study (recovery from weekend)
- **Tuesday:** Optional 1-1.5 hrs study (9-10:30pm) OR gym only
- **Wednesday:** Gym only, no study (mid-week recovery)
- **Thursday:** Optional 1-1.5 hrs study (9-10:30pm) OR gym only
- **Friday:** Full rest (no gym, no study) + 30 min weekly check-in (evening)
- **Subtotal:** 0-3 hours (optional/conditional)

**Weekend Power Sessions (Sat-Sun):**
- **Saturday:** 4-5 hours deep study (mental prime time) + gym
- **Sunday:** 4-5 hours study + gym
- **Subtotal:** 8-10 hours

**Weekly Total:** 10-13 hours (weekends drive 80% of progress)

### Phase 2: After Engineering Job Starts (Jan 2026 onwards)

**Months 1-3 (Onboarding):**
- **Work:** Full focus on learning new job (40hrs/week paid learning)
- **Weekends:** 0-3 hours if energy permits (NO PRESSURE)
- **Gym:** Maintain 5-6x/week (mental health priority)
- **Goal:** Stabilize, learn team workflows, survive onboarding

**Months 4-19 (Stable + Supplemental Learning):**
- **Work:** Apply engineering skills, gain experience (40hrs/week)
- **Weekends:** 5-8 hours data-specific study (Sat 3-4hrs, Sun 2-4hrs)
- **Gym:** Maintain 5-6x/week (non-negotiable)
- **Focus:** SQL â†’ Cloud â†’ Data tools progression
- **Flexible:** Reduce hours when work is demanding

---

## TIMELINE FLEXIBILITY

**Original Plan:** 9 months (Nov 2025 â†’ July 2026)

**Updated Philosophy:** 18-24 months (Nov 2025 â†’ mid-2026 to mid-2027)

**Why the change:**
- Better to spend 1-2 years becoming a STRONG engineer in a good role
- Then specialize in data engineering from solid foundation
- Rushing to "Data Engineer" title in 9 months = shallow skills
- Strong foundation now = better DE candidate later

**What this means:**
- Learning plan remains the same (all phases, projects, certs)
- Job search targets Junior SWE/Platform roles (not just stepping stones)
- Timeline extends if you land good engineering role
- Skills matter more than speed to title

**Your Timeline at a Glance:**

**Learning Track (Continues as Planned):**
```
Nov 2025: Python Foundations (4 weeks)
â”œâ”€ W1: âœ… Complete (25%)
â”œâ”€ W2: ðŸ“ In Progress (target 50%)
â”œâ”€ W3: Project 1 planning
â””â”€ W4: Project 1 complete + recovery week

Dec 2025 onwards: Continue learning progression
â”œâ”€ SQL & Data Analysis
â”œâ”€ AWS & Cloud
â”œâ”€ Databricks + Capstone
â””â”€ All 6 projects + 3 certifications
```

**Job Search Track (Updated):**
```
Nov-Dec 2025: Junior SWE/Platform Engineer search
â”œâ”€ W2-3: Applications + networking
â”œâ”€ W4-6: Follow-ups + interviews
â””â”€ Target: Offer by Jan 2026

Jan 2026 - Mid-2027: Engineering Role (18-24 months)
â”œâ”€ Learn to code properly (40hrs/week paid)
â”œâ”€ Build strong foundation
â”œâ”€ Continue learning track on weekends (5-8hrs/week)
â”œâ”€ Complete projects and certifications
â””â”€ Become strong engineer with DE specialization

Mid-2027: Data Engineer Transition
â”œâ”€ Apply to DE roles with 18-24mo engineering experience
â”œâ”€ Strong portfolio (projects + certs + real job experience)
â”œâ”€ Solid coding foundation from Junior SWE/Platform role
â””â”€ Target: Â£60k-Â£75k DE offer
```

**Philosophy Update:**
Better to spend 18-24 months becoming a STRONG engineer, then specialize in data, than to rush into 'Data Engineer' title in 9 months with shallow foundation.

---

## PHASE-BY-PHASE BREAKDOWN

### PHASE 1: PYTHON FOUNDATIONS
**Duration:** Weeks 1â€“8 (Nov 22, 2025 â€“ Jan 12, 2026)  
**Status:** Week 1 IN PROGRESS

**Learning Goal:** Master Python fundamentals, establish sustainable rhythm

**Primary Resource:**
- Bogdan Python Course (O'Reilly) - Chapters 1â€“54+
- Current: ~25â€“30%
- Phase target: ~70% by end of Week 8 (Jan 12)

**Method:**
- Code EVERY example yourself (no copy-paste)
- Take detailed notes on confusing concepts
- Build practice problems after each section
- Use Jupyter notebooks for interactive learning

**Weekly Micro-Targets (Satâ†’Fri weeks):**
- Week 1 (Nov 22â€“28): Consolidate to ~30%; select Project 1 idea
- Week 2 (Nov 29â€“Dec 5): 30% â†’ 45â€“50%; list 5â€“10 confusing topics
- Week 3 (Dec 6â€“12): 50% â†’ 60â€“65%; Project 1 planning + repo skeleton
- Week 4 (Dec 13â€“19): Recovery; reach ~70%; start coding Project 1
- Week 5 (Dec 20â€“26): Project 1 core logic working; error handling/tests
- Week 6 (Dec 27â€“Jan 2): Project 1 polish + README; finalize functionality
- Week 7 (Jan 3â€“9): Python consolidation; advanced topics review (iterators, generators, OOP)
- Week 8 (Jan 10â€“12): Buffer/recovery; Project 1 final touches; prepare for SQL phase

Use WEEK_BY_WEEK for day-by-day details.

**Parallel Activity:**
- Python for Data Analysis (McKinney) - Commute reading only, Chapters 1-5
- Job search: 30-company target list, CV update, LinkedIn optimization

**Deliverables:**
- [ ] Bogdan course 70% complete
- [ ] Python automation project on GitHub (file organizer, CSV analyzer, or log parser)
- [ ] CV updated with new positioning
- [ ] LinkedIn headline + about section updated
- [ ] 10 applications submitted (stepping-stone roles)
- [ ] Sustainable study rhythm established

**Success Metrics:**
- Can write Python functions confidently without syntax lookup
- Understand data structures (lists, dicts, sets, tuples)
- Implement basic OOP (classes, methods, attributes)
- Handle file I/O and errors properly
- Use Git/GitHub independently

---

### PHASE 2: SQL & DATA ANALYSIS (SQL Mastery)
**Duration:** Weeks 9â€“16 (Jan 18 â€“ Mar 9, 2026)  
**Status:** Not started yet

**Learning Goal:** Master SQL fundamentals + complex queries, build first ETL analysis

**Primary Resource:**
- Mode Analytics SQL Tutorial (https://mode.com/sql-tutorial/) - FREE
- Topics: SELECT, WHERE, JOINs, GROUP BY, window functions, CTEs, subqueries
- Estimated time: 12-15 hours

**Secondary:**
- Frank Kane ML Course (commute watching, not deep study)
- Python for Data Analysis Chapters 6-8 (reference reading)

**Weekly Targets (Satâ†’Fri):**
- Week 9 (Jan 18â€“24): SQL Basics â€” SELECT/WHERE/ORDER BY; 20 practice queries
- Week 10 (Jan 25â€“31): Joins Deep Dive â€” INNER/LEFT/RIGHT/FULL; 25 join queries
- Week 11 (Feb 1â€“7): Aggregations â€” GROUP BY/HAVING; multi-table summaries (20 queries)
- Week 12 (Feb 8â€“14): Window Functions I â€” ROW_NUMBER/RANK/LAG/LEAD (20 queries)
- Week 13 (Feb 15â€“21): Window Functions II + CTEs/Subqueries; first visualization
- Week 14 (Feb 22â€“28): Performance basics (EXPLAIN, indexes); refactor queries
- Week 15 (Mar 1â€“7): Project 2 finish â€” README + visualizations + push to GitHub
- Week 16 (Mar 8â€“13): Buffer/recovery â€” ensure 30+ queries, tighten docs

**Project 2: Financial Dataset Analysis**
- Load financial dataset (Kaggle or similar) into PostgreSQL/SQLite
- Write 20+ analytical queries
- Create simple visualizations (pandas + matplotlib)
- Export results and document insights
- Push to GitHub with comprehensive README

**Deliverables:**
- [ ] Mode Analytics SQL Tutorial complete
- [ ] PostgreSQL/SQLite installed and comfortable with
- [ ] 30+ SQL practice queries written
- [ ] Project 2 complete and on GitHub
- [ ] 20-25 total stepping-stone applications submitted
- [ ] 2-3 informational interviews conducted

**Success Metrics:**
- Write complex queries with multiple JOINs confidently
- Use window functions (ROW_NUMBER, RANK, LAG/LEAD)
- Understand when to use CTEs vs subqueries
- Analyze real datasets and extract insights
- Design SQL queries that run efficiently

---

### PHASE 3: AWS & CLOUD
**Duration:** Weeks 17â€“24 (Mar 15 â€“ May 10, 2026)  
**Status:** Not started yet  
**Study Intensity:** 12â€“15 hours/week (certification sprint near AWS exam)

**Learning Goal:** Earn AWS Data Engineer â€“ Associate; build AWS data pipeline

**Primary Resource:**
- AWS official DEA-C01 materials; handsâ€‘on in AWS Free Tier; practice exams

**Certification Details:**
- **Name:** AWS Certified Data Engineer â€“ Associate (DEAâ€‘C01)
- **Cost:** ~$150 USD
- **Format:** 65 questions, scenario-based (130 min)
- **Pass Score:** 720/1000 (72%)
- **Exam Date Target:** Week 24 (by May 10, 2026)

**Study Plan (example):**
- **Weeks 17â€“20:** Core AWS services (S3, Glue, Athena, Redshift, Lambda); build labs
- **Weeks 21â€“23:** Practice exams + weak area drilling; finalize Project 3 (AWS pipeline)
- **Week 24:** Exam week (by Sun May 10)

**Project 3: AWS Data Pipeline**
- Data lake on S3; transformations via Lambda/Glue; warehouse on Redshift/RDS
- EventBridge/Step Functions for orchestration; CloudWatch for monitoring
- IaC template (CloudFormation/Terraform) and architecture diagram

**Job Search Activity:**
- New role secured and settled in (3-4 weeks in)
- Continue learning without external job search pressure
- No stepping-stone applications this phase

**Deliverables:**
- [ ] AWS Data Engineer â€“ Associate PASSED âœ… (by Week 24)
- [ ] Project 3 (AWS Data Pipeline) complete and documented
- [ ] 3+ practice exams scored â‰¥ 75â€“80%
- [ ] IaC template + architecture diagram committed

**Success Metrics:**
- Pass AWS exam on first or second attempt
- Design data lakes on S3 with proper partitioning
- Build serverless ETL with Lambda/Glue
- Work effectively with Redshift/Athena
- Implement monitoring and alerting (CloudWatch/SNS)

---

### PHASE 4: DATABRICKS + CAPSTONE
**Duration:** Weeks 25â€“36 (May 17 â€“ Aug 3, 2026)  
**Status:** Not started yet  
**Study Intensity:** 12â€“16 hours/week (mix of Databricks + project build)

**Learning Goal:** Complete Databricks medallion project and Capstone

**Primary Resource:**
- Databricks Academy materials + Capstone build

Optional: Databricks Professional certification (schedule flexibly during Phase 4)

**Focus Areas:**
- Databricks ETL (Bronze/Silver/Gold), workflows, monitoring
- Capstone: Commodity Price Monitoring System (Weeks 25â€“36)

**Project 3 Continued:**
- Complete Gold layer implementation
- Add advanced Delta features (time travel, schema evolution)
- Implement data quality framework
- Add monitoring and alerting
- Polish documentation

**Deliverables:**
- [ ] Databricks Professional certification PASSED âœ…
- [ ] Project 3 fully complete and documented
- [ ] 2 Databricks certifications obtained ðŸŽ‰
- [ ] Production-ready medallion architecture demonstrated

**Success Metrics:**
- Pass Professional exam on first or second attempt
- Design production data pipelines independently
- Understand advanced Delta Lake features
- Build multi-hop architectures
- Optimize Spark jobs for performance

---

### (Legacy sections removed): Former Phase 5â€“7 content consolidated into Phase 3 (AWS) and Phase 4 (Databricks + Capstone). Job search launch now explicitly occurs in Week 36 within Phase 4.

---

## PROJECT LIBRARY (AUTHORITATIVE)

### Project 1: Python Automation Tool
- **Phase:** 1 (Nov-Dec)
- **Tech Stack:** Python, file I/O, error handling, argparse
- **Purpose:** Demonstrate Python fundamentals
- **Scope:** File organizer, CSV analyzer, or log parser
- **Deliverable:** GitHub repo with working tool + comprehensive README
- **Time:** 8-10 hours total

### Project 2: Financial Dataset Analysis
- **Phase:** 2 (Dec-Jan)
- **Tech Stack:** SQL, PostgreSQL/SQLite, Python (pandas), matplotlib
- **Purpose:** Demonstrate SQL mastery + data analysis capability
- **Scope:** Load financial data, write 20+ queries, create visualizations
- **Deliverable:** GitHub repo with queries, analysis, insights documented
- **Time:** 10-12 hours total

### Project 3: Databricks Medallion ETL Pipeline
- **Phase:** 3-4 (Jan-Feb)
- **Tech Stack:** PySpark, Delta Lake, Databricks, Databricks Workflows
- **Purpose:** Demonstrate medallion architecture + Delta Lake understanding
- **Scope:** Bronze/Silver/Gold layers, data quality, orchestration
- **Deliverable:** Databricks notebooks + GitHub with architecture diagram
- **Time:** 15-18 hours total

### Project 4: ML Classification Pipeline
- **Phase:** 5 (Mar)
- **Tech Stack:** Python, scikit-learn, pandas, model evaluation
- **Purpose:** Demonstrate ML + DE integration
- **Scope:** Financial prediction (fraud or credit risk), end-to-end pipeline
- **Deliverable:** GitHub repo with model card, evaluation metrics, code
- **Time:** 12-15 hours total

### Project 5: AWS Data Pipeline
- **Phase:** 5 (Apr)
- **Tech Stack:** AWS (S3, Lambda, Glue, Redshift), Python
- **Purpose:** Demonstrate cloud data engineering capability
- **Scope:** Ingest â†’ Transform â†’ Load pipeline, serverless preferred
- **Deliverable:** GitHub repo with CloudFormation/Terraform + architecture diagram
- **Time:** 15-18 hours total

### Project 6: CAPSTONE - Commodity Price Monitoring System
- **Phase:** 4 (Weeks 25â€“36, May 17â€“Aug 3)
- **Tech Stack:** Python, PySpark, Delta Lake, Databricks, APIs
- **Purpose:** Demonstrate full-stack end-to-end capability
- **Scope:** Production-grade system with ingestion, ETL, quality, monitoring, docs
- **Deliverable:** Complete repo with notebooks, architecture, demo video
- **Time:** 25-30 hours total

---

## CERTIFICATIONS (AUTHORITATIVE)

### Databricks Certified Data Engineer Associate
- **Target Date:** January 2026 (Week 13)
- **Cost:** ~$200 USD
- **Format:** Multiple choice + code scenarios, 90 minutes
- **Pass Score:** 70%
- **Topics:** PySpark, Delta Lake, Databricks SQL, Workflows
- **Study Time:** 30-40 hours (including hands-on labs)
- **Retake Buffer:** 2 weeks if needed

### Databricks Certified Data Engineer Professional
- **Target Date:** February 2026 (Week 15)
- **Cost:** ~$200 USD
- **Format:** Scenario-based, architecture-focused, 120 minutes
- **Pass Score:** 70%
- **Topics:** Advanced Spark, streaming, multi-hop architecture, optimization
- **Study Time:** 30-40 hours (including hands-on labs)
- **Retake Buffer:** 2 weeks if needed

### AWS Certified Data Engineer â€“ Associate
- **Target Date:** May 2026 (Week 24)
- **Cost:** ~$150 USD
- **Format:** 65 multiple choice questions, scenario-based, 130 minutes
- **Pass Score:** 720/1000 (72%)
- **Exam Code:** DEA-C01
- **Topics:** S3, Glue, Athena, Redshift, Kinesis, Lambda, IAM
- **Study Time:** 40-50 hours (including hands-on labs)
- **Retake Buffer:** 2 weeks if needed

---

## KEY RESOURCES (VERIFIED ONLY)

### Python
- **Bogdan Python Course** (O'Reilly)
  - Comprehensive Python fundamentals
  - Real course, real chapters (don't fabricate chapter titles)
  
- **Python for Data Analysis** (Wes McKinney, O'Reilly)
  - Reference for pandas, numpy, data manipulation
  - Chapters 1-5 for commute reading

### SQL
- **Mode Analytics SQL Tutorial** (https://mode.com/sql-tutorial/)
  - FREE resource
  - Basic, Intermediate, Advanced sections
  - Hands-on practice with real datasets

### Databricks
- **Databricks Academy** (official)
  - Data Engineer Associate course
  - Data Engineer Professional course
  - Free for community members
  - Real hands-on labs in Databricks Community Edition

### AWS
- **AWS Certified Data Engineer â€“ Associate Study Materials**
  - Official AWS study guide (from AWS training)
  - AWS Free Tier for hands-on labs
  - Practice exams available

---

## CONTINGENCY PLANS

### If Behind by 1-2 Weeks
- Add 2-3 hours to next 2 weekends
- Focus on core content only
- Reduce project polish (working > perfect)

### If Behind by 3+ Weeks
- Extend current phase by 1-2 weeks
- Maintain sustainable pace (quality over speed)
- Adjust timeline as needed

### If Certification Exam Failed
- Review results (weak areas)
- Use 2-week retake buffer
- Focus study on identified gaps
- Most people pass on second attempt

### If No Stepping-Stone Offer by February
- Stay at MSP current role
- Adjust timeline: Reduce stepping-stone search to maintenance level
- Focus energy on certifications
- Launch Data Engineer search with 2 Databricks certs
- This is acceptable - you'll have 2 strong credentials

### If Burnout Occurs
- Take full week off (guilt-free)
- Resume at 50% capacity
- Extend timeline as needed
- Health > timeline always

### If Job Search Takes Longer
- No financial pressure to accept wrong offer
- Continue applying 5-10/week
- Adjust strategy based on interview feedback
- Quality of next role > speed of exit

---

## WEEKLY CHECK-IN TEMPLATE

**Report Every Friday Evening:**

```
Week [X] Check-In
Status: [Learning Phase/Week X of Y]

Learning Progress:
- Resource: [What you studied]
- Completion: [%] â†’ [%]
- Topics covered: [List]
- Hours: [Actual] / [Target]

Projects:
- [Project name]: [Status - Planning/In Progress/Complete]
- Blockers: [Any challenges]

Job Search (if active):
- Applications: [#] submitted this week
- Interviews: [#] scheduled/completed
- Key learnings: [What you learned]

Wins (What went well):
- [Win 1]
- [Win 2]

Challenges (What was hard):
- [Challenge 1]
- [Challenge 2]

Next Week:
- Target: [Specific goal]
- Focus: [What you'll prioritize]
- Hours planned: [Expected]

Energy Level: [1-5 scale]
On Track: [Yes/No/Adjusting]
Notes: [Anything else]
```

---

## TRACKING & ACCOUNTABILITY

- Create spreadsheet: Week | Phase | Hours | Progress | Projects | Status
- Update weekly after Friday check-in
- Color code: Green (on track), Yellow (slightly behind), Red (needs adjustment)

### Monthly Reviews
- End of each month: Review all weekly check-ins
- Calculate total hours vs target
- Assess project completion
- Identify patterns
- Adjust next month if needed

### Accountability Partners
- **Claude:** Weekly check-ins, technical guidance, roadmap adjustments
- **Optional:** Data engineering Discord/Slack for community
- **Optional:** Study buddy or mentor for peer accountability

---

## CRITICAL SUCCESS FACTORS

### Non-Negotiables
1. **Gym 5-6 days/week** - Mental health foundation (you've proven this works)
2. **Weekend focus** - 80% of study happens Sat/Sun when mentally fresh
3. **Code everything yourself** - No copy-paste, build real neural pathways
4. **Weekly check-ins** - Accountability and course correction
5. **Build > Study** - 70% hands-on projects, 20% courses, 10% theory

### What NOT to Do
- Don't start new courses mid-phase (focus on core only)
- Don't polish projects endlessly (working > perfect)
- Don't skip recovery weeks (they're part of the strategy)
- Don't apply to jobs before May 2026 for Data Engineer roles
- Don't sacrifice sleep or meals for study

### What TO Do
- Show up every weekend consistently
- Ask for help when stuck >30 minutes
- Celebrate wins along the way
- Adjust intelligently when life happens
- Trust the process

---

## NEXT IMMEDIATE ACTIONS (This Week)

### Learning Track
- [ ] Bogdan Python: Code every example yourself
- [ ] Take detailed notes on confusing sections
- [ ] Target (Week 1): Reach ~30% by Friday Nov 28

### Job Search Track
- [ ] Update CV with new positioning (Cloud Support Engineer angle)
- [ ] Update LinkedIn headline: "Systems Engineer | Azure/M365 | Learning Python & Data | Cloud Support Focus"
- [ ] Update LinkedIn About section with career transition narrative
- [ ] Create 30-company target list (research 10 this week)
- [ ] Request 5 LinkedIn endorsements (Azure, M365, PowerShell, troubleshooting, Cloud)

- [ ] Friday evening (Nov 28): Report progress to Claude
- [ ] Confirm next week's focus
- [ ] Celebrate this week's wins

---

## FINAL COMMITMENT

```
I, Adil Hafeez, commit to:

Learning Track:
- Following this roadmap Nov 2025 â†’ Jul 2026
- Studying 10-13 hours/week minimum
- Building 6 real projects
- Earning 3 certifications
- Not quit when it gets hard
- Coding every example myself

Job Search Track:
- Starting stepping-stone search NOW
- Applying 2-5 roles/week when actively searching
- Networking consistently
- Treating interviews professionally
- Securing stepping-stone offer by December 2025

Both Tracks:
- Weekly Sunday check-ins with Claude
- Maintaining gym routine (5-6 days/week)
- Being kind to myself when I stumble
- Celebrating wins along the way
- Trusting this process

Signature: _______________________
Date: November 18, 2025
```

---

**Version:** 1.0 - Authoritative Single Source  
**Created:** November 18, 2025  
**Status:** Week 1 In Progress  
**Next Review:** Weekly (Every Friday)
